{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML summarizers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYxj3VhKuCi2"
      },
      "source": [
        "def summarizer(text, option, fraction):\n",
        "    # \"Tf-IDF-Based\", \"Frequency-Based\", \"Gensim-Based\"\n",
        "    \n",
        "    frac=fraction\n",
        "    if option == \"TfIdf-Based\":\n",
        "        return tfidf_based(text, frac)\n",
        "    if option == \"Frequency-Based\":\n",
        "        return freq_based(text, frac)\n",
        "    if option == \"Gensim-Based\":\n",
        "        doc=nlp(text)\n",
        "        text=\"\\n\".join([sent.text for sent in doc.sents])\n",
        "        return gensim_based(text=text, ratio=frac)\n",
        "def tfidf_based(msg,fraction=0.3):\n",
        "    # Creating Pipeline\n",
        "    doc=nlp(msg)\n",
        "    \n",
        "    #Sent_tokenize\n",
        "    sents =[sent.text for sent in doc.sents]\n",
        "    \n",
        "    #Number of Sentence User wants\n",
        "    num_sent=int(np.ceil(len(sents)*fraction))\n",
        "    \n",
        "    #Creating tf-idf removing the stop words matching token pattern of only text\n",
        "    tfidf=TfidfVectorizer(stop_words='english',token_pattern='(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "    X=tfidf.fit_transform(sents)\n",
        "    \n",
        "    #Creating a df with data and tf-idf value\n",
        "    df=pd.DataFrame(data=X.todense(),columns=tfidf.get_feature_names())\n",
        "    indexlist=list(df.sum(axis=1).sort_values(ascending=False).index)\n",
        "#     indexlist=list((df.sum(axis=1)/df[df>0].count(axis=1)).sort_values(ascending=False).index)\n",
        "    \n",
        "    # Subsetting only user needed sentence\n",
        "    needed = indexlist[:num_sent]\n",
        "    \n",
        "    #Sorting the document in order\n",
        "    needed.sort()\n",
        "    \n",
        "    #Appending summary to a list--> convert to string --> return to user\n",
        "    summary=[]\n",
        "    for i in needed:\n",
        "        summary.append(sents[i])\n",
        "    summary=\"\".join(summary)\n",
        "    summary = summary.replace(\"\\n\",'')\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNDZSLP4uEaV"
      },
      "source": [
        "def freq_based(text, fraction):\n",
        "    # Convert to pipeline\n",
        "    doc = nlp(text)\n",
        "    # Break to sentences\n",
        "    sentence = [sent for sent in doc.sents]\n",
        "    # Number of sentence user wants\n",
        "    numsentence = int(np.ceil(fraction*len(sentence)))\n",
        "\n",
        "    # Tokenizing and filtering key words\n",
        "    words = [word.text.lower()\n",
        "             for word in doc.doc if word.is_alpha and word.is_stop == False]\n",
        "    # Converting to df for calculating weighted frequency\n",
        "    df = pd.DataFrame.from_dict(\n",
        "        data=dict(Counter(words)), orient=\"index\", columns=[\"freq\"])\n",
        "    df[\"wfreq\"] = np.round(df.freq/df.freq.max(), 3)\n",
        "    df = df.drop('freq', axis=1)\n",
        "\n",
        "    # Convert weighted frequency back to dict\n",
        "    wfreq_words = df.wfreq.to_dict()\n",
        "\n",
        "    # Weight each sentence based on their wfreq\n",
        "    sent_weight = []\n",
        "    for sent in sentence:\n",
        "        temp = 0\n",
        "        for word in sent:\n",
        "            if word.text.lower() in wfreq_words:\n",
        "                temp += wfreq_words[word.text.lower()]\n",
        "        sent_weight.append(temp)\n",
        "    wdf = pd.DataFrame(data=np.round(sent_weight, 3), columns=['weight'])\n",
        "    wdf = wdf.sort_values(by='weight', ascending=False)\n",
        "    indexlist = list(wdf.iloc[:numsentence, :].index)\n",
        " # Summary\n",
        "    sumlist = []\n",
        "    for s in indexlist[:5]:\n",
        "        sumlist.append(sentence[s])\n",
        "    summary = ''.join(token.string.strip() for token in sumlist)\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}